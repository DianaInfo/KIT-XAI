\documentclass{report}

\usepackage{color}
\usepackage[margin=1in]{geometry}
\usepackage{soul}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amssymb}
\usepackage{enumerate}
\usepackage{xcolor}

\newcommand{\asw}[2][teal]{}
% Folgende Zeile auskommentieren, wenn Antworten nicht gewünscht sind.
\renewcommand{\asw}[2][teal]{\textcolor{#1}{#2}}

\newcommand{\com}[2][blue]{\textcolor{#1}{#2}}
\newcommand{\qst}[2][red]{\textcolor{#1}{#2}}
\newcommand{\tab}{\hspace*{5mm}}
\newcommand{\todo}[2][red]{\textcolor{#1}{TODO: #2}}

\usepackage{contour}
\usepackage{ulem}

\renewcommand{\ULdepth}{1.5pt}
\contourlength{0.8pt}

\newcommand{\myuline}[1]{%
	\uline{\phantom{#1}}%
	\llap{\contour{white}{#1}}%
}

%opening
\title{XAI: Explainable Artificial Intelligence}
\author{Diana Burkart}

\begin{document}
	
	\maketitle
	\newpage
	
	\tableofcontents
	\newpage
	
	\chapter{Hints}
	
	\begin{itemize}
		\item An "E:" in the beginning marks questions from the example exam questions of the lecture.
	\end{itemize}
	
	\chapter{Klausurfragen}
	
	\section{General}
	
	\begin{itemize}
		\item Why do we need XAI?
		\asw{\newline It helps to understand and debug the behavior of learned models. also it answers the question WHY a model behaves the way it does.}
		\item What parts define a dataset?
		\asw{\newline inputs, features, targets, datapoints}
		\item What are the 2 classes of interpretability?
		\asw{\newline intrinsic interpretability \& post-hoc interpretability (mixture also possible)}
		\item E: How does model complexity influence interpretability?
		\asw{\newline With growing model complexity the interpretability of models decreases. This often is due to the many factors that are included in models with higher complexity.}
	\end{itemize}

	\section{Intrinsic Interpretability}
	
	\begin{itemize}
	\item What are the methods for intrinsic interpretability?
	\asw{\newline Linear Regression, Logistic Regression, Generalized Linear Model (GLM) \& Generalized Additive Model (GAM)}
	
	\item What is \textbf{\underline{Linear Regression}}?
	\asw{\newline Linear Regression wants to predict a continuous value with a linear combination of features.
	\newline $y = x^T \beta + \epsilon$}
	\item What does the $\epsilon$ in the Linear Regression equation express?
	\asw{\newline It expresses the uncertainty / noise on $y$.}
	\item What are methods to calculate Linear Regression?
	\asw{\newline It can be calculated by OLS (Ordinary Least Squares) and TLS (Total Least Squares).
	\newline OLS: $\hat{\beta} = argmin_\beta ||y - X \beta||^2_2$, $\hat{\beta} = (X^T X^{-1})^{-1} X^T y$
	\newline TLS: includes uncertainty in features}
	\item What is the interpretation of Linear Regression?
	\asw{\newline Changing a single feature $x_m$ by $\Delta x_m$ will directly effect outcome $y$ by $\beta_m \Delta x_m$
		\newline Assumption: no interaction between features}
	\item What is the $R^2$ value and how is it defined?
	\asw{\newline $R^2$ expresses how well the model explains the data
		\newline Definition: $R^2 = 1 - \frac{SSE}{SST} = 1 - \frac{\sum_{i} (y_i - \hat{y}_i)^2}{\sum_i (y_i - \overline{y}_i)^2}$}
	\item E: How can the thresholds for the $R^2$ metric for a regression model be interpreted?
	\asw{\newline $R^2 \rightarrow 1$: model fits data very well
		\newline $R^2 \rightarrow 0$: model does not fit data at all
		\newline $R^2 < 0$: model learned opposite (worse than data mean)}
	\item How can the t-statistic be used for interpretation of a NN?
	\asw{\newline It can be used for each feature to interpret its performance.}
	\item What are the assumptions regarding a NN interpretation for Linear Regression?
	\asw{\newline Linearity (prediction linear in features),
		\newline Normality (data is normally distributed),
		\newline Homoscedasticity (error of prediction has const. variance),
		\newline Independence (of datapoints),
		\newline Fixed Features (no measurment error in features),
		\newline Absence of multicollinearity}
	\item What are types of Regularization?
	\asw{\newline Ridge Regression: $\hat{\beta} = argmin_\beta[||y - X\beta||^2_2 + \lambda ||\beta||^2_2]$ (L2-norm)
		\newline $\rightarrow$ having many features $\rightarrow$ higher chance for strong correlation
		\newline LASSO Regression: $\hat{\beta} = argmin_\beta[||y - X\beta||^2_2 + \lambda ||\beta||_1]$ (L1-norm)
		\newline $\rightarrow$ \underline{L}east \underline{A}bsolute \underline{S}hrinkage and \underline{S}election \underline{O}perator, encourages sparsity in features}
	\item Why do we need Regularization?
	\asw{\newline It is needed to make sure that only the most important features are used to determine the results.
		\newline \qst{What else?}}
	\item What are advantages/ diadvantages of Linear Regression?
	\asw{\newline \textcolor{green}{$+$} Easy to understand
		\newline \textcolor{green}{$+$} Widely applied / well understood
		\newline \textcolor{green}{$+$} Easy to assess individual feature contribution
		\newline \textcolor{green}{$+$} Guaranteed to find optimal solution (w.r.t. given data)
		\newline \textcolor{red}{$-$} Can only model linear relationships between features \& output
		\newline \textcolor{red}{$-$} Limited predictive performance
		\newline \textcolor{red}{$-$} Weights can be unintuitive}
	\newline
	\hrule 
	
	\item What are \textbf{\underline{Generalized Linear Models}}?
	\asw{\newline They constist of a linear predictor $\eta = x^T \beta$, a distribution (models expectation $\mu = \mathbb{E}[y]$ and a link function $g(\mu) = \eta$, $g^{-1}(\eta) = \mu$)}
	\item Which assumption is not valid for GLMs, but for Linear Regression?
	\asw{\newline Normality}
	\item What is the link function with GLMs?
	\asw{\newline \qst{link function?}}
	\item What are advantages/ disadvantages of GLMs?
	\asw{\newline \textcolor{green}{$+$} No longer restricted to normality assumption
		\newline \textcolor{green}{$+$} Can train models on non-linear data (although limited) while maintaining linear relationship between data \& weights
		\newline \textcolor{red}{$-$} Link function complicates interpretability of weights \& features
		\newline \textcolor{red}{$-$} Still restricted to linear relationships between data \& weights}
	\newline
	\hrule 
	
	\item What is \textbf{\underline{Logistic Regression}}?
	\asw{\newline It uses a linear model for classification}
	\item What is the difference between Logistic and Linear Regression?
	\asw{\newline Linear Regression predicts a continuous value Logistic Regression is used for classification. Logistic Regression uses a sigmoid or softmax function at the end.}
	\item What task does Logistic Regression solve?
	\asw{\newline Classification (not Regression)}
	\item Why can't we regress directly on data? What is added to solve this issue?
	\asw{\newline When regressing directly on the data the decision boundary is changed a lot, when new points are added. Therefore the sigmoid function is added at the end.}
	\item What are advantages/ disadvantages of Logistic Regression?
	\asw{\newline \textcolor{green}{$+$} Widely applied, well understood
		\newline \textcolor{green}{$+$} Weights are somewhat interpretable
		\newline \textcolor{green}{$+$} Provides probabilities for classification
		\newline \textcolor{red}{$-$} No automatic feature interaction
		\newline \textcolor{red}{$-$} Complete separation if one feature perfectly separates the classes $\rightarrow$ it would theoretically get infinite weight
		\newline \tab $\rightarrow$ makes fitting of model impossible, paradox: a single great feature makes model unusable}
	\item What is the Perceptron?
	\asw{\newline Perceptron is a simple calculation-unit that applies logistic regression.}
	\item What is the formula to calculate the Perceptron?
	\asw{\newline $\mu = \frac{1}{1+ exp(-x^T \beta)}$, $y = a(x^T \beta)$}
	\item Why do we need an activation function?
	\asw{\newline The activation function makes it possible to work with non-linear data. Also it can output probabilities for classification.}
	\item What kinds of activation functions exist?
	\asw{\newline Sigmoid: $a(z) = \frac{1}{1+ exp(-z)}$, ReLU: $a(z) = max(0,z)$
		\newline classical perceptrons $\rightarrow$ heavyside step function: $a(z) = 0_{[z\geq0]}, 1_{[z<0]}$}
	\item E: Given a logistic regression model with trained weights $\beta$, how does the output $y$ change, if we change a single input feature $x'_m$ by one unit, i.e., $x'_m = x_m + 1$?
	\asw{\newline $y' = $
		\newline \qst{A change of $x_m$ by 1 unit changes the ratio by a factor of $exp(\beta_m)$}}
	\newline
	\hrule 
	
	\item What are \textbf{\underline{Generalized Additive Models}}?
	\asw{\newline They consist of smooth functions per feature that are summed up and build the link function.}
	\item What is the difference between GLMs and GAMs?
	\asw{\newline The simple weight coefficients of GLMs are replaced with functions for each features in GAMs.}
	\item How is the link function for GAMs defined? \qst{Does the link function need to be a smooth function?}
	\asw{\newline The link function has to be a smooth function, but does not have to be specified precisely.}
	\item Does the link function have to be specified precisely?
	\asw{\newline It does not have to be specified precisely.}
	\item What happens if prediction is conditioned on only one single feature $x_k$?
	\asw{\newline In this case the conditioned feature is fixed and the expectation is only calculated on all other features.}
	\item What is Backfitting?
	\asw{\newline Backfitting always calculates $y$ with the feature functions as they are and then adjusts one the feature functions one after each other until it converges and the feature functions don't change anymore.}
	\item Can the Least Squares method still be used with GAMs?
	\asw{\newline Yes, the Least Squares method can still be used with GAMs.}
	\item E: What are the advantages/ disadvantages of GAMs?
	\asw{\newline \textcolor{green}{$+$} Model complex relationships between features \& prediction
		\newline \textcolor{green}{$+$} Smooth functions allow non-linear, non-monotonic \& interactive effects to be captured
		\newline \textcolor{green}{$+$} Provide valuable insights into direction \& significance of feature effects, even in non-linear relationships
		\newline \textcolor{red}{$-$} Interpretation of results can be challenging due to complexity of smooth functions
		\newline \textcolor{red}{$-$} Selecting suitable smooth functions can be challenging}
	
	\end{itemize}

	\section{Model Types}
	
	\begin{itemize}
		\item What different model types exist?
		\asw{\newline }
		
		\item What are \textbf{\underline{Decision Trees}}?
		\asw{\newline }
		\item How do Decision Trees learn?
		\asw{\newline }
		\item How is the "best" boundary for Decision Trees chosen?
		\asw{\newline }
		\item How can the purity of nodes be mesasured?
		\asw{\newline }
		\item E: What are possible stopping criteria for Decision Trees?
		\asw{\newline e.g. max. depth, max instances per nodes}
		\item What is an alternative for stopping conditions?
		\asw{\newline Pruning}
		\item What is Random Forest?
		\asw{\newline }
		\item What are the two methods used in Random Forest models?
		\asw{\newline Bagging, Boosting}
		\item How can the feature importance for Random Forest be calculated?
		\asw{\newline }
		\item How can interaction of feature be represented by Random Forest models?
		\asw{\newline }
		\item What are the advantages/ disadvantages of Decision Trees?
		\asw{\newline }
		\item What is the difference between Random Forest and Decision Trees?
		\asw{\newline }
		\item What is meant by feature interaction?
		\asw{\newline }
		\item E: How can feature importance be computed for a decision tree?
		\asw{\newline }
		
		\item What are the downsides of the algorithms before NNs?
		\asw{\newline Expressiveness / Representation Power, scalability to higher dimensions, manual feature selection}
		\item What are the disadvantages of NNs?
		\asw{\newline No intrinsic interpretability anymore}
		
		\item What are \textbf{\underline{GNNs}}?
		\asw{\newline }
		\item For which application are GNNs suitable?
		\asw{\newline e.g. molecules, (social) networks, NLP, images}
		\item E: Name two examples for graph like structures and epxlain what the nodes and edges represent.
		\asw{\newline }
		\item What are graph-related problems?
		\asw{\newline Node classification, graph classification, link prediction, community detection, anomaly detection}
		\item What are the advantages of graph data?
		\asw{\newline permutation invariant}
		\item What is Message Passing (MP)?
		\asw{\newline }
		\item What are the steps for MP?
		\asw{\newline }
		\item E: For message passing, what kind of property is required for the learned aggregation function?
		\asw{\newline }
		\item What are methods for GNNs?
		\asw{\newline Graph Convolutional Network, Graph Attention Networks, Gated Graph Sequence Networks}
		
		\item What are \textbf{\underline{CNNs}}?
		\asw{\newline }
		\item What are the hyperparameters relevant to CNNs?
		\asw{\newline }
		\item What is Pooling? Why are we need Pooling?
		\asw{\newline }
		\item How many parameters do Pooling layers have?
		\asw{\newline }
		\item How many parameters do Convolution layers have?
		\asw{\newline }
		\item In addition to Convolutional Layers, which two parts make up a typical convolution stage in a CNN? Name them and describe their function.
		\asw{\newline }
		\item What parts are needed for the convolution stage?
		\asw{\newline convolution, ReLU, Pooling}
		\item What are possible application areas of CNNs?
		\asw{\newline image classification, object detection, image segmentation (deconvolutions)}
		
		\item What are \textbf{\underline{Transformer}} Networks?
		\asw{\newline }
		\item What are possible application areas of Transformer network?
		\asw{\newline Computer Vision, NLP, Speech, Translation, Robotics, Image Classfication, Multimodal Generation, Text Generation}
		\item What was used before the introduction of Transformer for the input of sequences?
		\asw{\newline RNNs (LSTM, GRU)}
		\item What are problems with the use of RNNs for sequential inputs?
		\asw{\newline sequential processing, localization, single direction context}
		\item What is Attention?
		\asw{\newline }
		\item What is the intuition behind Transformers?
		\asw{\newline Query, Key, Value}
		\item How is the Attention computed?
		\asw{\newline 1) compute attention score, normalize, softmax
		\newline 2) sum attention score weighted by values
		\newline $\rightarrow$ $z = softmax(\frac{Q \times K'}{\sqrt{d_{key}}}) \times V$}
		\item What part of the Tranformer architecture is used for what?
		\asw{\newline only decoder $\rightarrow$
		\newline only encoder $\rightarrow$
		\newline both $\rightarrow$}
		\item How is the input of the Transformer handled beforehand?
		\asw{\newline tokenization of inputs, positional encoding (permutation invariance, sequence processing)}
		\item How does the positional encoding of the Transformer input work?
		\asw{\newline }
		\item How does the tokenization of the Transformer input work?
		\asw{\newline }
		\item What types of modalities can be input to an Transformer model?
		\asw{\newline }
		\item What is the MHSA?
		\asw{\newline Multi-headed Self-Attention: Conceptualized Embeddings, Self-Attention, Multiple Attention Heads, Weighting with Linear Layers}
		\item What is the Tokenwise-MLP?
		\asw{\newline }
		\item What are Residual Updates and why do we need them?
		\asw{\newline }
		\item What is Layer Normalization and why do we need it?
		\asw{\newline }
		\item How many of the layers are stacked on top of each other in the Transformer network?
		\asw{\newline commonly 6 or 12}
		\item How is the output layer of the Transformer network defined?
		\asw{\newline }
		\item Do the inputs for the Transformer network need padding? Is the input length for the Transformer network fixed?
		\asw{\newline }
		\item What different types of Attention are used in the Transformer network and how do they work? In which part of the network are they used?
		\asw{\newline Self-Attention, Cross-Attention, Masked-Attention}
		\item What properties do Transformer networks have?
		\asw{\newline expressive, optimizable, efficient}
		\item E: Name two weakness of Recurrent Neural Networks (RNNs), which are addressed by the Transformer Architecture.
		\asw{\newline }
		\item E: What three matrices are required for computing attention? On a high level, what is their purpose?
		\asw{\newline }
		\item What is special about the latent space learned by the CLIP foundation model?
		\asw{\newline }
		
		\item What is \textbf{\underline{CLIP}}?
		\asw{\newline Contrastive Language-Image Pre-Training}
		\item What is CLIP used for?
		\asw{\newline pretraining image and text encoders}
		\item What types of data does CLIP receive?
		\asw{\newline }
		\item What are the parts of a CLIP network? What types of networks are often used for them?
		\asw{\newline }
		\item How can the different modalities be mapped to check for similarities?
		\asw{\newline }
		\item What is the distance in the latent space equal to?
		\asw{\newline }
		\item What is Self-Supervised Training?
		\asw{\newline }
		\item What are the steps for Self-Supervised Training in CLIP?
		\asw{\newline }
		\item What are possible applications of CLIP?
		\asw{\newline }
		\item How can CLIP models be explained?
		\asw{\newline }
		
		\item E: What is a \textbf{\underline{Diffusion}} networks?
		\asw{\newline }
		\item E: What are the two motivations for the research question of ”Uncovering the hidden language of Diffusion models”?
		\asw{\newline }
		
	\end{itemize}

	\section{Post-hoc Interpretability}
	
	\begin{itemize}
	\item E: Name one local and one global interpretation method and explain what makes the local/global.
	\asw{\newline }
	
	\item Please explain \textbf{\underline{Permutation Features importance}}.
	\asw{\newline }
	\item Please explain LIME.
	\asw{\newline }
	\item E: What are advantages and disadvantages of LIME.
	\asw{\newline }
	\item Please explain Gloabl Surrogate.
	\asw{\newline }
	\item E: Briefly explain how to create a Global Surrogate.
	\asw{\newline }
	
	\item Please explain \textbf{\underline{Shapley Values}}.
	\asw{\newline }
	\item E: Briefly explain, what interpretation do Shapley Values provide. What ”question” do they answer?
	\asw{\newline }
	\item Please explain SHAP.
	\asw{\newline }
	
	\end{itemize}
	
	\section{Prototype Learning}
	
	\begin{itemize}
	\item What are \textbf{\underline{Prototypes}}?
	\asw{\newline }
	\item What is Prototype Learning?
	\asw{\newline }
	\item What kinds of Prototype Learning exist?
	\asw{\newline }
	\item What is the definition of a Prototype with respect to interpretable machine learning models, e.g. for computer vision?
	\asw{\newline }
	\item How are prototypes related to Bag of (visual) words?
	\asw{\newline }
	\item Describe a network architecture for non-learnable Prototypes.
	\asw{\newline }
	\end{itemize}
	
	\section{SCG: Scene Graph Generation}
	
	\begin{itemize}
	\item What is a \textbf{\underline{Scene Graph}}?
	\asw{\newline }
	\item What is meant by Pruning Edges?
	\asw{\newline }
	\end{itemize}

	\section{Guest Lectures (Applied XAI)}
	
		\subsection{XAI in Energy Systems}
		
		\begin{itemize}
		\item E: What XAI technique is currently predominantly used for Energy Systems?
		\asw{\newline }
		\end{itemize}
	
		\subsection{XAI in Material Science}
		
		\begin{itemize}
		\item 
		\end{itemize}
	
		\subsection{XAI in Computer Security}
		
		\begin{itemize}
		\item E: Name three types of explanation aware attacks
		\asw{\newline }
		\item E: How can we protect a model against explanation aware attacks using trigger patches?
		\asw{\newline }
		\end{itemize}
	
		\subsection{XAI in Mobility}
		
		\begin{itemize}
		\item What is the MAB-EX framework?
		\item What do the components of the MAB-EX framework do?
		\item Why do we need explanations also for non-AI components?
		\end{itemize}
	
		\subsection{XAI in NLP}
		
		\begin{itemize}
		\item E: Briefly Describe the two main steps of Pertubation-based Quality Estimation in NLP.
		\asw{\newline }
		\end{itemize}
	
	\section{Neural Network Interpretation}
	
	\begin{itemize}
		\item E: Briefly describe the three main steps of Network Dissecton.
		\asw{\newline }
		\item E: Which part of a Convolution Stage in a CNN might cause problems for saliency maps and why?
		\asw{\newline }
		\item E: Name one advantage and one disadvantage for Saliency Maps.
		\asw{\newline }
	\end{itemize}
	
	
\end{document}
