\documentclass{report}

\usepackage{color}
\usepackage[margin=1in]{geometry}
\usepackage{soul}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amssymb}
\usepackage{enumerate}
\usepackage{xcolor}

\newcommand{\asw}[2][teal]{}
% Folgende Zeile auskommentieren, wenn Antworten nicht gewünscht sind.
\renewcommand{\asw}[2][teal]{\textcolor{#1}{#2}}

\newcommand{\com}[2][blue]{\textcolor{#1}{#2}}
\newcommand{\qst}[2][red]{\textcolor{#1}{#2}}
\newcommand{\tab}{\hspace*{5mm}}
\newcommand{\todo}[2][red]{\textcolor{#1}{TODO: #2}}

\usepackage{contour}
\usepackage{ulem}

\renewcommand{\ULdepth}{1.5pt}
\contourlength{0.8pt}

\newcommand{\myuline}[1]{%
	\uline{\phantom{#1}}%
	\llap{\contour{white}{#1}}%
}

%opening
\title{XAI: Explainable Artificial Intelligence}
\author{Diana Burkart}

\begin{document}
	
	\maketitle
	\newpage
	
	\tableofcontents
	\newpage
	
	\chapter{Hints}
	
	\begin{itemize}
		\item An "E:" in the beginning marks questions from the example exam questions of the lecture.
		\item No formulars in exams, only general understanding of content
	\end{itemize}
	
	\chapter{Klausurfragen}
	
	\section{General}
	
	\begin{itemize}
		\item Why do we need XAI?
		\asw{\newline It helps to understand and debug the behavior of learned models. also it answers the question WHY a model behaves the way it does.}
		\item What parts define a dataset?
		\asw{\newline inputs, features, targets, datapoints}
		\item What are the 2 classes of interpretability?
		\asw{\newline intrinsic interpretability \& post-hoc interpretability (mixture also possible)}
		\item E: How does model complexity influence interpretability?
		\asw{\newline With growing model complexity the interpretability of models decreases. This often is due to the many factors that are included in models with higher complexity.}
	\end{itemize}
	\newpage

	\section{Intrinsic Interpretability}
	
	\begin{itemize}
	\item What are the methods for intrinsic interpretability?
	\asw{\newline Linear Regression, Logistic Regression, Generalized Linear Model (GLM) \& Generalized Additive Model (GAM)}
	
	\item What is \textbf{\underline{Linear Regression}}?
	\asw{\newline Linear Regression wants to predict a continuous value with a linear combination of features.
	\newline $y = x^T \beta + \epsilon$}
	\item What does the $\epsilon$ in the Linear Regression equation express?
	\asw{\newline It expresses the uncertainty / noise on $y$.}
	\item What are methods to calculate Linear Regression?
	\asw{\newline It can be calculated by OLS (Ordinary Least Squares) and TLS (Total Least Squares).
	\newline OLS: $\hat{\beta} = argmin_\beta ||y - X \beta||^2_2$, $\hat{\beta} = (X^T X^{-1})^{-1} X^T y$
	\newline TLS: includes uncertainty in features}
	\item What is the interpretation of Linear Regression?
	\asw{\newline Changing a single feature $x_m$ by $\Delta x_m$ will directly effect outcome $y$ by $\beta_m \Delta x_m$
		\newline Assumption: no interaction between features}
	\item What is the $R^2$ value and how is it defined?
	\asw{\newline $R^2$ expresses how well the model explains the data
		\newline Definition: $R^2 = 1 - \frac{SSE}{SST} = 1 - \frac{\sum_{i} (y_i - \hat{y}_i)^2}{\sum_i (y_i - \overline{y}_i)^2}$}
	\item E: How can the thresholds for the $R^2$ metric for a regression model be interpreted?
	\asw{\newline $R^2 \rightarrow 1$: model fits data very well
		\newline $R^2 \rightarrow 0$: model does not fit data at all
		\newline $R^2 < 0$: model learned opposite (worse than data mean)}
	\item How can the t-statistic be used for interpretation of a NN?
	\asw{\newline It can be used for each feature to interpret its performance.}
	\item What are the assumptions regarding a NN interpretation for Linear Regression?
	\asw{\newline Linearity (prediction linear in features),
		\newline Normality (data is normally distributed),
		\newline Homoscedasticity (error of prediction has const. variance),
		\newline Independence (of datapoints),
		\newline Fixed Features (no measurment error in features),
		\newline Absence of multicollinearity}
	\item What are types of Regularization?
	\asw{\newline Ridge Regression: $\hat{\beta} = argmin_\beta[||y - X\beta||^2_2 + \lambda ||\beta||^2_2]$ (L2-norm)
		\newline $\rightarrow$ having many features $\rightarrow$ higher chance for strong correlation
		\newline LASSO Regression: $\hat{\beta} = argmin_\beta[||y - X\beta||^2_2 + \lambda ||\beta||_1]$ (L1-norm)
		\newline $\rightarrow$ \underline{L}east \underline{A}bsolute \underline{S}hrinkage and \underline{S}election \underline{O}perator, encourages sparsity in features}
	\item Why do we need Regularization?
	\asw{\newline It is needed to make sure that only the most important features are used to determine the results. It helps keeping the weights as small as possible. It reduces the complexity of the model, so that it only generally fits the data and not overfits to the data.}
	\item What are advantages/ diadvantages of Linear Regression?
	\asw{\newline \textcolor{green}{$+$} Easy to understand
		\newline \textcolor{green}{$+$} Widely applied / well understood
		\newline \textcolor{green}{$+$} Easy to assess individual feature contribution
		\newline \textcolor{green}{$+$} Guaranteed to find optimal solution (w.r.t. given data)
		\newline \textcolor{red}{$-$} Can only model linear relationships between features \& output
		\newline \textcolor{red}{$-$} Limited predictive performance
		\newline \textcolor{red}{$-$} Weights can be unintuitive}
	\newline
	\hrule 
	
	\item What are \textbf{\underline{Generalized Linear Models}}?
	\asw{\newline They constist of a linear predictor $\eta = x^T \beta$, a distribution (models expectation $\mu = \mathbb{E}[y]$ and a link function $g(\mu) = \eta$, $g^{-1}(\eta) = \mu$)}
	\item Which assumption is not valid for GLMs, but for Linear Regression?
	\asw{\newline Normality}
	\item What is the link function with GLMs?
	\asw{\newline Invertable function out of exponential family. \qst{link function?}}
	\item What are advantages/ disadvantages of GLMs?
	\asw{\newline \textcolor{green}{$+$} No longer restricted to normality assumption
		\newline \textcolor{green}{$+$} Can train models on non-linear data (although limited) while maintaining linear relationship between data \& weights
		\newline \textcolor{red}{$-$} Link function complicates interpretability of weights \& features
		\newline \textcolor{red}{$-$} Still restricted to linear relationships between data \& weights}
	\newline
	\hrule 
	
	\item What is \textbf{\underline{Logistic Regression}}?
	\asw{\newline It uses a linear model for classification}
	\item What is the difference between Logistic and Linear Regression?
	\asw{\newline Linear Regression predicts a continuous value Logistic Regression is used for classification. Logistic Regression uses a sigmoid or softmax function at the end.}
	\item What task does Logistic Regression solve?
	\asw{\newline Classification (not Regression)}
	\item Why can't we regress directly on data? What is added to solve this issue?
	\asw{\newline When regressing directly on the data the decision boundary is changed a lot, when new points are added. Therefore the sigmoid function is added at the end.}
	\item What are advantages/ disadvantages of Logistic Regression?
	\asw{\newline \textcolor{green}{$+$} Widely applied, well understood
		\newline \textcolor{green}{$+$} Weights are somewhat interpretable
		\newline \textcolor{green}{$+$} Provides probabilities for classification
		\newline \textcolor{red}{$-$} No automatic feature interaction
		\newline \textcolor{red}{$-$} Complete separation if one feature perfectly separates the classes $\rightarrow$ it would theoretically get infinite weight
		\newline \tab $\rightarrow$ makes fitting of model impossible, paradox: a single great feature makes model unusable}
	\item What is the Perceptron?
	\asw{\newline Perceptron is a simple calculation-unit that applies logistic regression.}
	\item What is the formula to calculate the Perceptron?
	\asw{\newline $\mu = \frac{1}{1+ exp(-x^T \beta)}$, $y = a(x^T \beta)$}
	\item Why do we need an activation function?
	\asw{\newline The activation function makes it possible to work with non-linear data. Also it can output probabilities for classification.}
	\item What kinds of activation functions exist?
	\asw{\newline Sigmoid: $a(z) = \frac{1}{1+ exp(-z)}$, ReLU: $a(z) = max(0,z)$
		\newline classical perceptrons $\rightarrow$ heavyside step function: $a(z) = 0_{[z\geq0]}, 1_{[z<0]}$}
	\item E: Given a logistic regression model with trained weights $\beta$, how does the output $y$ change, if we change a single input feature $x'_m$ by one unit, i.e., $x'_m = x_m + 1$?
	\asw{\newline $y' = y \times exp(\beta_m)$
		\newline A change of $x_m$ by 1 unit changes the ratio by a factor of $exp(\beta_m)$}
	\newline
	\hrule 
	
	\item What are \textbf{\underline{Generalized Additive Models}}?
	\asw{\newline They consist of smooth functions per feature that are summed up and build the link function.}
	\item What is the difference between GLMs and GAMs?
	\asw{\newline The simple weight coefficients of GLMs are replaced with functions for each features in GAMs.}
	\item How is the link function for GAMs defined? \qst{Does the link function need to be a smooth function?}
	\asw{\newline The link function has to be a smooth function, but does not have to be specified precisely.}
	\item Does the link function have to be specified precisely?
	\asw{\newline It does not have to be specified precisely.}
	\item What happens if prediction is conditioned on only one single feature $x_k$?
	\asw{\newline In this case the conditioned feature is fixed and the expectation is only calculated on all other features.}
	\item What is Backfitting?
	\asw{\newline Backfitting always calculates $y$ with the feature functions as they are and then adjusts one the feature functions one after each other until it converges and the feature functions don't change anymore.}
	\item Can the Least Squares method still be used with GAMs?
	\asw{\newline Yes, the Least Squares method can still be used with GAMs.}
	\item E: What are the advantages/ disadvantages of GAMs?
	\asw{\newline \textcolor{green}{$+$} Model complex relationships between features \& prediction
		\newline \textcolor{green}{$+$} Smooth functions allow non-linear, non-monotonic \& interactive effects to be captured
		\newline \textcolor{green}{$+$} Provide valuable insights into direction \& significance of feature effects, even in non-linear relationships
		\newline \textcolor{red}{$-$} Interpretation of results can be challenging due to complexity of smooth functions
		\newline \textcolor{red}{$-$} Selecting suitable smooth functions can be challenging}
	
	\end{itemize}
	\newpage

	\section{Model Types}
	
	\begin{itemize}
		\item What different model types exist?
		\asw{\newline Decision Trees (Random Forest), Neural Network, GNN, CNN, Transformer, CLIP, Diffusion Models}
		
		\item What are \textbf{\underline{Decision Trees}}?
		\asw{\newline They define boundaries for features that decide between different subtrees.}
		\item How do Decision Trees learn?
		\asw{\newline They always pick the currently "best" boundary and continue their search in the subtrees of this boundary.}
		\item How is the "best" boundary for Decision Trees chosen?
		\asw{\newline It is chosen by the following algorithm:
			\newline 1) sort all feature values
			\newline 2) candidate threshold: build avg between 2 subsequent values
			\newline 3) each pair (feature \& candidate threshold) is potential boundary}
		\item How can the purity of nodes be mesasured? \qst{Formulas?}
		\asw{\newline Information Gain (higher $\widehat{=}$ better)
			\newline Gini Index (lower $\widehat{=}$ better)
			\newline MSE (for regression tasks)
			\newline $\rightarrow$ Learning smallest optimal decision tree is a NP-complete problem.}
		\item E: What are possible stopping criteria for Decision Trees?
		\asw{\newline e.g. max. depth, max instances per nodes, purity}
		\item What is an alternative for stopping conditions?
		\asw{\newline Pruning: completely build up your tree and then aggregate subtrees if they are not necessary.}
		\item What is Random Forest?
		\asw{\newline It is an a way to combine many decision trees to build one decision.}
		\item What are the two methods used in Random Forest models?
		\asw{\newline Bagging: learn different trees on subsets of training data (sampling with replacement)
			\newline Boosting: learn different trees on reweighted versions of training data}
		\item How can the feature importance for Random Forest be calculated?
		\asw{\newline The contribution of each feature can be measured by dividing it by the measure for all features.}
		\item How can interaction of feature be represented by Random Forest models?
		\asw{\newline Representing feature interaction is only limited possible and no complex depencies can be represented.}
		\item What are the advantages/ disadvantages of Decision Trees?
		\asw{\newline \textcolor{green}{$+$} Somewhat captures interactions
			\newline \textcolor{green}{$+$} Data often grouped in understandable criteria
			\newline \textcolor{green}{$+$} Trees create "good" explanations
			\newline \textcolor{red}{$-$} Linear relationship only approximated
			\newline \textcolor{red}{$-$} Lack of smoothness: small feature change can cause significant change in prediction
			\newline \textcolor{red}{$-$} Unstable: changes in data easily yield very different trees
			\newline \textcolor{red}{$-$} Size/depth essential to interpretation}
		\item What is the difference between Random Forest and Decision Trees?
		\asw{\newline Random Forest combines many Decision Trees for one decision.}
		\item What is meant by feature interaction?
		\asw{\newline It describes that a decision depends on a combination of features.}
		\item E: How can feature importance be computed for a decision tree?
		\asw{\newline feature importance can be computed by how often and with which impact a feature helps with the decision. Features that split earlier have in general more impact than later ones.}
		\newline
		\hrule 
		
		\item What are the downsides of the algorithms before NNs?
		\asw{\newline Expressiveness / Representation Power, scalability to higher dimensions, manual feature selection}
		\item What are the disadvantages of NNs?
		\asw{\newline No intrinsic interpretability anymore}
		\newline
		\hrule 
		
		\item What are \textbf{\underline{GNNs}}?
		\asw{\newline They are NNs that work on Graph structured data. Graph structured data consists of nodes and edges.}
		\item For which application are GNNs suitable?
		\asw{\newline e.g. molecules, (social) networks, NLP, images}
		\item E: Name two examples for graph like structures and epxlain what the nodes and edges represent.
		\asw{\newline 1) Molecules are graph like structures. Their nodes represent the atoms and the edges represent the connections between them.
			\newline 2) Networks can have nodes as people and edges represent the people who know each other.}
		\item What are graph-related problems?
		\asw{\newline Node classification, graph classification, link prediction, community detection, anomaly detection}
		\item What are the advantages of graph data?
		\asw{\newline permutation invariant}
		\item What is Message Passing (MP)?
		\asw{\newline Message Passing describes the process of passing messages along edges from one node to another. Every nodes sends a message (its embedding) to all other neighbors.}
		\item What are the steps for MP?
		\asw{\newline 1) Node gathers messages (embeddings) from neighbors and its own.
			\newline 2) Aggregate messages using permutation invariant function (e.g. sum)
			\newline 3) Result is passed through learnable function (e.g. MLP)}
		\item E: For message passing, what kind of property is required for the learned aggregation function?
		\asw{\newline It has to be permutation invariant.}
		\item What are methods for GNNs?
		\asw{\newline Graph Convolutional Network, Graph Attention Networks, Gated Graph Sequence Networks}
		\newline
		\hrule 
		
		\item What are \textbf{\underline{CNNs}}?
		\asw{\newline A type of NN that can work with images as input.}
		\item What are the hyperparameters relevant to CNNs?
		\asw{\newline number of filters, kernel (filter) size, stride, padding}
		\item What is Pooling? Why are we need Pooling?
		\asw{\newline Pooling reduces the size of layers, it is used to capture spatial information. There are different types of Pooling like Average or Max Pooling.}
		\item How many parameters do Pooling layers have?
		\asw{\newline Pooling does not have any learned parameters. It needs 2 hyperparameters, kernel size and stride.}
		\item How many parameters do Convolution layers have?
		\asw{\newline Convolutional layers have $K \times K \times D_1 \times F$ parameters.}
		\item E: In addition to Convolutional Layers, which two parts make up a typical convolution stage in a CNN? Name them and describe their function.
		\asw{\newline Convolutional Stage = Convolutional layers + Activation Function (ReLU) + Pooling}
		\item What parts are needed for the convolution stage?
		\asw{\newline convolution, ReLU, Pooling}
		\item What are possible application areas of CNNs?
		\asw{\newline image classification, object detection, image segmentation (deconvolutions)}
		\item What are typical architectures of CNNs?
		\asw{\newline e.g. ResNet, InceptionNet, HRNet}
		\newline
		\hrule 
		
		\item What are \textbf{\underline{Transformer}} Networks?
		\asw{\newline Transformer work with any input (mostly text, often also images) to create an output. Transformer are mostly based on the Attention mechanism. The order of the input sequence is encoded into an positional encoding.}
		\item What are possible application areas of Transformer network?
		\asw{\newline Computer Vision, NLP, Speech, Translation, Robotics, Image Classfication, Multimodal Generation, Text Generation}
		\item What was used before the introduction of Transformer for the input of sequences?
		\asw{\newline RNNs (LSTM, GRU)}
		\item What are problems with the use of RNNs for sequential inputs?
		\asw{\newline sequential processing, localization, single direction context}
		\item What is Attention?
		\asw{\newline It describes at what a network is currently looking at while computing a specific output. It sets the words in relation to each other and can describe what is currently important.}
		\item What is the intuition behind Transformers?
		\asw{\newline Query (like a search term, token),
			\newline Key (short result summary/identification),
			\newline Value (content of results)
			\newline each token searches the database for relevant token and updates its representation based on most relevant token}
		\item How is the Attention computed?
		\asw{\newline 1) compute attention score, normalize, softmax
		\newline 2) sum attention score weighted by values
		\newline $\rightarrow$ $z = softmax(\frac{Q \times K'}{\sqrt{d_{key}}}) \times V$}
		\item What part of the Tranformer architecture is used for what?
		\asw{\newline only encoder $\rightarrow$ e.g. BERT (predict masked word)
		\newline only decoder $\rightarrow$ e.g. GPT (predict next word)
		\newline both $\rightarrow$ e.g. TS (translation)}
		\item How is the input of the Transformer preprocessed?
		\asw{\newline tokenization of inputs, positional encoding (permutation invariance, sequence processing)}
		\item How does the positional encoding of the Transformer input work?
		\asw{\newline The positional encoding leads to permutation invariance (attention) and helps with the sequence processing. The idea is to add intelligently specific values on the embedded inputs, those could be the values of the sinus function.}
		\item How does the tokenization of the Transformer input work?
		\asw{\newline Depending on the input modality simple linear layers or specialized pre-trained embeddings (NLP) are used. It takes the word as input and converts it into a number or vector.}
		\item What types of modalities can be input to an Transformer model?
		\asw{\newline Images, Text, etc.}
		\item What is the MHSA?
		\asw{\newline Multi-headed Self-Attention: Conceptualized Embeddings, Self-Attention, Multiple Attention Heads, Weighting with Linear Layers}
		\item What is the Tokenwise-MLP?
		\asw{\newline Every latent token updated individually using MLP: $z_i = w_z GeLU(W_1 x + b_1) + b_2$. Stores the "world knowledge".}
		\item What are Residual Updates and why do we need them?
		\asw{\newline The MLP computes a residual value that is added to the initial input. This stabilizes the training and allows for deeper NNs. (ResNet-block)}
		\item What is Layer Normalization and why do we need it?
		\asw{\newline It is similar to Batch normalization. It normalizes the distribution of an intermediate layer. It is often applied before attention and linear layers. It helps to stabilize the training.}
		\item How many of the layers are stacked on top of each other in the Transformer network?
		\asw{\newline Commonly there are 6 or 12. More layers enable better abstractions.}
		\item How is the output layer of the Transformer network defined?
		\asw{\newline It depends on the application and is therefore flexible. Commonly it's a linear layer head with an arbirtrary activation function. Often not all output tokens have to be used.}
		\item Do the inputs for the Transformer network need padding? Is the input length for the Transformer network fixed?
		\asw{\newline The input has padded to a fixed input length with the vanilla Transformer.}
		\item What different types of Attention are used in the Transformer network and how do they work? In which part of the network are they used?
		\asw{\newline Self-Attention (encoding): K, V from same input sequence
			\newline Cross-Attention (decoding): K, V from different sequence than Q
			\newline Masked-Attention (decoding): only lookback possible for token (rest masked)}
		\item What properties do Transformer networks have?
		\asw{\newline expressive, optimizable, efficient}
		\item E: Name two weakness of Recurrent Neural Networks (RNNs), which are addressed by the Transformer Architecture.
		\asw{\newline sequential processing, localization: hidden token mostly influenced by most recent token}
		\item E: What 3 matrices are required for computing attention? On a high level, what is their purpose?
		\asw{\newline Query, Key and Value weight matrices are needed. Query is supposed to work like a search term to find relevant tokens. Key helps to identify the tokens that are relevant. Value describes the content of an token.}
		\newline
		\hrule 
		
		\item What is \textbf{\underline{CLIP}}?
		\asw{\newline Contrastive Language-Image Pre-Training}
		\item What is CLIP used for?
		\asw{\newline Pretraining of image and text encoders}
		\item What types of data does CLIP receive?
		\asw{\newline The input are images and text.}
		\item What are the parts of a CLIP network? What types of networks are often used for them?
		\asw{\newline text-encoder: Transformer, vision-encoder: ResNet or ViT (Vision Transformer)}
		\item How can the different modalities be mapped to check for similarities?
		\asw{\newline Both modality encodings are mapped to a shared latent space. There they can be checked for similarity.}
		\item What is the distance in the latent space equal to?
		\asw{\newline It equals cosine-similarity.}
		\item What is Self-Supervised Training?
		\asw{\newline It uses the data as input and tries to predict another part of the same data. This could be the next word in a sentence for example.}
		\item What are the steps for Self-Supervised Training in CLIP?
		\asw{\newline 1) encode image-text-pairs in the shared latent space
			\newline 2) compute cosine-similarity for all pairs of a batch
			\newline 3) compute contrastive loss function (similar close, dissimilar far apart in latent space)}
		\item What are possible applications of CLIP?
		\asw{\newline Robotics: open-vocabulary object detection, plan actions in environment and solve tasks}
		\item How can CLIP models be explained?
		\asw{\newline They can have an inner monologue (embodied reasoning). The latent space can be interpreted for explanations.}
		\item E: What is special about the latent space learned by the CLIP foundation model?
		\asw{\newline The latent space is shared between different modalities and encode similar inputs close to each other.}
		\newline
		\hrule 
		
		\item E: What is a \textbf{\underline{Diffusion}} networks?
		\asw{\newline Diffusion networks are generative models that take noise as input and gradually create images from them through optimization.}
		\item What are Score-based Diffusion Generative Models?
		\asw{\newline They iteratively generate new samples through a denoising process beginning with random Gaussian noise. They try to fit underlying data distribution and synthesize new samples from this distribution.}
		\item What is the score of Score-based Diffusion Generative Models referring to?
		\asw{\newline The model tries to learn a score for the PDF of the underlying data distribution.}
		\item What is a needed property for the normalization constant $Z$?
		\asw{\newline $Z$ needs to be tractable. That is not easy.}
		\item How does sampling from a score-based Diffusion model work?
		\asw{\newline New samples are generated using iterative Langevin Dynamics procedure.}
		\item What is the Fisher Divergence Training Objective?
		\asw{\newline }
		\item What issues occure with sampling?
		\asw{\newline Samples can get stuck, because the noise can start in a low density region and does not know the direction, because there is no good estimate of the score in this area.}
		\item What are Noise-conditioned Score Networks?
		\asw{\newline }
		\item What is the weighted Fisher Divergence Training objective?
		\asw{\newline }
		\item What are the advantages of perturbing data with an SDE?
		\asw{\newline }
		\item How is the Forward Diffusion SDE defined?
		\asw{\newline }
		\item How is the Reverse Generative Diffusion SDE defined?
		\asw{\newline }
		\item What are the advantages / disadvantages of Probability flow ODEs?
		\asw{\newline }
		\item What are Latent / Stable Diffusion models?
		\asw{\newline They can synthesize high-resolution images. They use diffusion in latent space for significant speed-up in Sampling and Learning. They use a pre-trained encoder/decoder. They use flexible conditioning with Cross-Attention and condition the generation process on various inputs (not same inputs)}
		\item What is meant with the Hidden Language of Diffusion models?
		\asw{\newline }
		\item What is the algorithm of the conceptor for the Hidden Language of Diffusion models?
		\asw{\newline (1): Extract a training set of 100 images from the model with the concept prompt
			\newline (2): MLP maps each word embedding wi to a coefficient f(wi)
			\newline (3): we compute the learned pseudo-token w*N as a linear combination of the vocabulary tokens
			\newline (4): We sample a random noise for each of the training images and noise the images accordingly
			\newline (5): We use the model to denoise the image with our pseudo-token w*N
			\newline (6): Compute 2 losses: reconstruction and sparsity loss
			\newline (7) In inference time, we only consider the top-50 tokens rated by their coefficients to reconstruct the concept images.}
		\item E: What are the two motivations for the research question of ”Uncovering the hidden language of Diffusion models”?
		\asw{\newline }
		\item How can the Hidden Language of Diffusion models be extracted?
		\asw{\newline }
		\item What are the key takeaways for the Hidden Language of Diffusion models?
		\asw{\newline Conceptor, Diffusion models, Homograph concepts (homograph: one word, multiple meanings), Bias detection}
		\item Why are there potential Privacy issues with Diffusion Generative models?
		\asw{\newline }
		\item How is it possible to extract training images from Diffusion models?
		\asw{\newline }
		\item What are the keytake aways regarding privacy in Diffusion models?
		\asw{\newline Memorization Image-Generation Models
			\newline Privacy concerns in Diffusion Models
			\newline Balancing Generative Power \& Privacy}
		
	\end{itemize}
	\newpage

	\section{Post-hoc Interpretability}
	
	\begin{itemize}
	\item What types of Post-hoc interpretability methods exist?
	\asw{\newline Model-agnostic vs. model-specific}
	\item Why do we need two types of Post-hoc interpretability methods?
	\asw{\newline Model-agnostic methods can cope with many different models and create explanations for them. Model-specific methods can provide explanations only for specific types of models, but therefore the explanations are more detailed and sometimes easier to interpret.}
	\item What are global methods?
	\asw{\newline They describe the average behavior of ML models.}
	\item What are examples of global methods?
	\asw{\newline Partial dependence plots, Accumulated local effect plots, Feature interaction (H-statistic), Functional decomposition
		\newline Permutation feature importance: measures the importance of a feature as an increase in loss when the feature is permuted
		\newline Global surrogate models: replaces the complex model with a simpler interpretable model
		\newline Prototypes: are representative data points of a distribution and can be used to enhance interpretability}
	\item What are local methods?
	\asw{\newline They describe individual predictions.}
	\item What are examples of local methods?
	\asw{\newline Individual conditional expectation curves, Scoped rules (anchors)
		\newline Local surrogate models (LIME): replace the complex model with a locally interpretable surrogate
		model
		\newline Counterfactual explanations: examine which features would need to be changed to achieve a desired prediction. (We might learn about this approach on Thursday)
		\newline Shapley values: fairly attribute the prediction to individual features.
		\newline $\rightarrow$ SHAP: is a computation method for Shapley values, that also offers global interpretation
		methods based on combinations of Shapley values across the data.}
	\item What is the difference between global and local methods?
	\asw{\newline While local methods concentrate on explaining specific predictions, global methods explain the overall behavior of the model.}
	\item E: Name one local and one global interpretation method and explain what makes the local/global.
	\asw{\newline \qst{To answer}}
	\newline
	\hrule 
	
	\item Please explain \textbf{\underline{Permutation Features Importance}}.
	\asw{\newline This methods performs a permutation of a feature and measures the importance of this feature depending on the increase in the error. If the error increases by a lot, the feature is important, otherwise it's irrelevant.}
	\item How to compute FI (feature importance)?
	\asw{\newline Can be computed as quotient or difference: $FI_m = \frac{L(y, \hat{f}(X^{(m)}_{perm}))}{L(y, \hat{f}(X))}$, $FI_m = L(y, \hat{f}(X^{(m)}_{perm})) - L(y, \hat{f}(X))$}
	\item Is Permutation Feature Importance a  local or global method?
	\asw{\newline Global.}
	\item Should FI be computed on the training or test set and which purpose does it have respectively?
	\asw{\newline }
	\item What are advantages / disadvantages of Permuation Feature Importance?
	\asw{\newline \textcolor{green}{$+$} Nice interpretation: FI is the increase in model error when feature's info is removed
		\newline \textcolor{green}{$+$} Highly compressed, global insight into model's behavior
		\newline \textcolor{green}{$+$} Very simple
		\newline \textcolor{red}{$-$} Requires ground truth
		\newline \textcolor{red}{$-$} Results might vary greatly depending on permutation
		\newline \textcolor{red}{$-$} Can be biased by unrealistic data instances}
	\newline
	\hrule 
	
	\item Please explain Gloabl Surrogate.
	\asw{\newline An interpretable model tries to imitate predictions of a blackbox model.}
	\item E: Briefly explain how to create a Global Surrogate.
	\asw{\newline Select a dataset and get predicitions from complex model. Pick an interpretable surrogate model and fit the interpretable model to the selected dataset and predictions of the complex model.}
	\item How is the quality of the surrogate model measured?
	\asw{\newline It is measured how good the surrogate model replicatees the predictions of the complex blackbox model. This is measured using the $R^2$ metric.}
	\item What are advantages / disadvantages of Global Surrogate?
	\asw{\newline \textcolor{green}{$+$} Very flexible, any interpretable model can be used
		\newline \textcolor{green}{$+$} Very intuitive
		\newline \textcolor{green}{$+$} $R^2$: How good is approximation?
		\newline \textcolor{red}{$-$} Surrogate never sees ground truth $\rightarrow$ learn something about model or data
		\newline \textcolor{red}{$-$} Inherits disadvantages of surrogate}
	\newline
	\hrule 
	
	\item Please explain LIME.
	\asw{\newline LIME is short for Local Interpretable Model-agnostic Explanations. It trains a local surrogate model to explain individual predictions.}
	\item Is LIME a local or global method?
	\asw{\newline LIME is a local method.}
	\item What is the formula for LIME?
	\asw{\newline $\psi(x) = armin_{g \in G} L(\hat{f}, g, \Xi_x) + \Omega(g)$}
	\item What is the algorithm for LIME?
	\asw{\newline 1) Select a prediction we want explained.
		\newline 2) Perturb input and get new predictions from complex model.
		\newline 3) Weight new samples (proximity).
		\newline 4) Train interpretable model on dataset with variations (local).
		\newline 5) Explain prediction by interpreting surrogate model.}
	\item E: What are advantages and disadvantages of LIME.
	\asw{\newline \textcolor{green}{$+$} Explanations short (selective) \& possibly contrastive
		\newline \textcolor{green}{$+$} Works for tabular data, text \& images
		\newline \textcolor{green}{$+$} Can use other features than original model was trained on
		\newline \textcolor{red}{$-$} Complexity of explanation model has to be pre-defined
		\newline \textcolor{red}{$-$} Instability of explanations
	}
	\newline
	\hrule 
	
	\item Please explain \textbf{\underline{Shapley Values}}.
	\asw{\newline They describe how good features of an instance would have performed without a particular feature.}
	\item E: Briefly explain, what interpretation do Shapley Values provide. What ”question” do they answer?
	\asw{\newline Question: "How would the other features of this particular instance have performed without that particular feature?"}
	\item What properties do Shapley Values satisfy?
	\asw{\newline Efficiency: contributions add up to difference of prediction and average prediction
		\newline Symmetry: contribution of 2 features same, if both contribute equally to all possible coalations
		\newline Dummy: value$=0$ $\rightarrow$ feature does not change prediction
		\newline Additivity: additively combined models $\rightarrow$ individual values additively combined}
	\item What is the formula of Shapley Values?
	\asw{\newline $\varphi_m = \frac{1}{M} \sum_{C_{/m}} \frac{\phi_m(C_{/m})}{\#|C_{/m}|}$
		\newline with marginal contribution of $m$ to $C_{/m}$: $\phi_m(C_{/m}) = \hat{f}(C_{/m} \cup m) - \hat{f}(C_{/m})$}
	\item How can the Shapley Value be calculated?
	\asw{\newline It can be approximated with the Monte-Carlo approach. The instance of interest $x$ is replaced with some features of another, random instance $z$. Feature m gets either replaced ($x_{-m}$) or not ($x_{+m}$)}.
	\item What are advantages / disadvantages of Shapley Values?
	\asw{\newline \textcolor{green}{$+$} fairly distributes differences between prediction \& avg prediction among feature values (LIME does not)
		\newline \textcolor{green}{$+$} allows contrastive explanations, comparison does not have to be avg prediction, can be subset, including single datapoint
		\newline \textcolor{green}{$+$} offers solid theory: efficiency, symmetry, dummy, additivity
		\newline \textcolor{red}{$-$} requires a lot of compute
		\newline \textcolor{red}{$-$} potentially misinterpreted, Shapley values do NOT measure change in prediction if feature was removed from training
		\newline \textcolor{red}{$-$} always uses all features, no sparse explanation
		\newline \textcolor{red}{$-$} inclusion of unrealistic instances (in training) $\rightarrow$ Frankenstein Samples}
	
	\item Please explain \textbf{SHAP}.
	\asw{\newline It is a combination of LIME and Shapley Values in an additive model.}
	\item Is SHAP a local or global method?
	\asw{\newline It is a local method.}
	\item What are the desirable properties for additive explanatory models that are required for fairness?
	\asw{\newline Local accuracy,
		\newline Missingness,
		\newline Consistency}
	\item What is the algorithm for SHAP?
	\asw{\newline 1) Maintain "background dataset" (representatives)
		\newline 2) Create "Frankenstein samples": replace features with features from other instances in dataset
		\newline 3) Evaluate "Frankenstein samples" and average outcome $\rightarrow$ outcome for particular set
		\newline 4) Repeat for several sets.
		\newline 5) Fit additive model using created dataset via weighted Linear Regression}
	\item What are advantages / disadvantages of SHAP?
	\asw{\newline \textcolor{green}{$+$} solid theoretical foundation
		\newline \textcolor{green}{$+$} inherits advantages of Shapley Values
		\newline \textcolor{green}{$+$} connects LIME \& Shapley Values
		\newline \textcolor{green}{$+$} global model interpretation
		\newline \textcolor{green}{$+$} fast implementations for specific models (e.g. TreeSHAP)
		\newline \textcolor{red}{$-$} KernelSHAP is slow
		\newline \textcolor{red}{$-$} KernelSHAP still ignores feature dependency
		\newline \textcolor{red}{$-$} inherits disadvantages from Shapley Values}
	
	\end{itemize}
	\newpage
	
	\section{Prototype Learning}
	
	\begin{itemize}
	\item What are \textbf{\underline{Prototypes}}?
	\asw{\newline Prototypes are the most predictive representation for a specific prediction result.}
	\item What can be types of Prototypes?
	\asw{\newline whole or part of a training sample, merged representation of multiple samples, encoded in feature space}
	\item How does Bag of Visual Words work?
	\asw{\newline Keypoint extraction, Cluster creation, Cluster comparison}
	\item What is a Prototype Layer?
	\asw{\newline It represents the distance of the encoding vector to Prototypes. Each node $\phi_n$ represents one prototype in Layer $\Phi$.
		\newline Computation: $z = Enc(x)$, $\Phi_j(z) = ||z - \phi_j||^2_2$}
	\item How is the Prototype Layer integrated into the Architecture?
	\asw{\newline The Prototype Layer $z = e(x)$ is located between the Encoding stage and the Decision stage.}
	\item How is the Prototype Layer trained?
	\asw{\newline It is optimized via backpropagation.}
	\item Are the Prototypes visualizable and interpretable?
	\asw{\newline The Prototypes are visualizable, but there are not interpretable by themselves.}
	\item What is Prototype Learning?
	\asw{\newline It describes the learning process of Prototypes that embody specific types of features.}
	\item What kinds of Prototypes exist?
	\asw{\newline Learnable, non-learnable Prototypes.}
	\item What kind of loss do Autoencoders use?
	\asw{\newline Reconstruction loss: $\mathcal{L}_{Rec} = \frac{1}{N} \sum_{i}^{N} ||(D \times E)(x_i) - x_i||^2_2$}
	\item What is the Auxiliary Head used for in Prototype Architectures?
	\asw{\newline It is an decoder that is not required for the prediction. It supports training by providing a second objective.}
	\item What types of loss exist in Reconstructable/Learnable Prototype Training?
	\asw{\newline Task-loss: $\mathcal{L}_{task}$, i.e. $\mathcal{L}_{CE}(y, \hat{y})$
		\newline Reconstruction-loss: $\mathcal{L}_{Rec} = \frac{1}{N} \sum_{i}^{N}||d(z_i) - x_i||^2_2$
		\newline R1-loss: $\mathcal{L}_{R1} = \frac{1}{M} \sum_{j=0}^{M} min_{i \in [0,N]} ||\phi_j - z_i||^2_2$
		\newline R2-loss: $\mathcal{L}_{R2} = \frac{1}{N} \sum_{i=0}^{N} min_{j \in [0,M]} ||z_i - \phi_j||^2_2$}
	\item Why is it important that Prototypes are diverse? How can diversity be encouraged?
	\asw{\newline Diverse Prototypes improve model performance and they encode more information together.
		\newline KL-Divergence, Jeffrey's Divergence}
	\item What are the downsides of Learnable Prototypes?
	\asw{\newline They are fixed in number and they do not stand in relation to each other.}
	\item How do non-learnable prototypes work?
	\asw{\newline They are not optimized via backpropagation. They are optimized via a separate clustering algorithm w.r.t. their similarities.}
	\item What are possible similarity measures for non-learnable Prototypes?
	\asw{\newline Prototype Cosine Similarity, Class Cosine Similarity}
	\item What kinds of loss exist for non-learnable Prototypes?
	\asw{\newline Task-loss: increase distance to incorrect class
		\newline Contrast-loss: increase distance to correct, but non-assigned protoype
		\newline Distance-loss: decrease distance to assigned prototype}
	\item E: What is the definition of a Prototype with respect to interpretable machine learning models, e.g. for computer vision?
	\asw{\newline \qst{What do they want to know?}}
	\item E: How are prototypes related to Bag of (visual) words?
	\asw{\newline With non-learnable Prototypes like in Bag of Visual Words clustering can be applied.}
	\item E: Describe a network architecture for non-learnable Prototypes.
	\asw{\newline There is an encoding stage then the bottleneck and afterwards the encoding stage. In the bottleneck layer the Prototypes are getting clustered by a separate clustering algorithm.}
	\end{itemize}
	\newpage
	
	\section{SCG: Scene Graph Generation}
	
	\begin{itemize}
	\item What is a \textbf{\underline{Scene Graph}}?
	\asw{\newline }
	\item How to get a Scene graph?
	\asw{\newline Detect objects and predict their relations.}
	\item What is meant by Pruning Edges?
	\asw{\newline It is a more efficient way of creating a scene graph. They create a Sparse graph from a Dense graph.}
	\item What network architecture is used for Pruning Edges?
	\asw{\newline Region Proposal Network}
	\item What are different types of edges between objects? \qst{not really types}
	\asw{\newline use class distribution of data, non-learnable, pre-processing of data needed}
	\item What steps are needed to generate a scene graph?
	\asw{\newline Extract bounding boxes and predict a label per box, predict relations between objects and display as graph.}
	\item Why are GNNs (Message Passing) not used for Scene Graphs?
	\asw{\newline limited expressive power, oversmoothing, bottleneck phenomena}
	\item How can the limitations of Message Passing networks be overcome and GNNs be used for Scene Graph generation?
	\asw{\newline use a continuous model for graphs (underlying graph) $\rightarrow$ Graph Neural Diffusion (GRAND)}
	
	\item What is \textbf{\underline{GRAND}}?
	\asw{\newline It is short for Graph Neural Diffusion.}
	\item What is the relation between GNNs and PDEs (Partial Differential Equations)?
	\asw{\newline }
	\item What are Spatial Diffusion Equations for Graphs?
	\asw{\newline $(div(z))_i = \sum_{j=1}^{nodes} w_{ij} z_{ij}$ with $w_{ij} = 1$, if $(i,j) \in E$ $\rightarrow$ sum of all edges assigned to node}
	\item What are the advantages of modeling GNNs through PDEs?
	\asw{\newline continuous underlying model for graph problems
		\newline with spatial operators on graphs, use temporal discretization to train GNNs
		\newline with spatial operators $\rightarrow$ linear system of ODEs (Ordinary Differential Equations), use established methods to solve ODEs.}
	\item What is the GRAND model/architecture?
	\asw{\newline }
	\item How are the edge and node features defined?
	\asw{\newline edge: difference between connected nodes, nodes: sum of edge features}
	\item How can GRAND be used for Scene Graph Generation (SGG)
	\asw{\newline simple: use GRAND instead of GNNs in already existing procedures,
		\newline OR: create fully connected graph from bounding box and labels, use GRAND to propagate info between nodes $\rightarrow$ then Pruning to remove unimportant edges and predict object relations
		\newline usage: create graph from superpixels
		\newline $\rightarrow$ graph $\rightarrow$ predict pixel classes $\rightarrow$ create graph with labels and all possible relations $\rightarrow$ GRAND for Scene Graph}
	\item What kind of explainability is used with Scene Graphs?
	\asw{\newline Goal-driven explainability}
	\end{itemize}
	\newpage

	\section{Guest Lectures (Applied XAI)}
	
		\subsection{XAI in Energy Systems}
		
		\begin{itemize}
		\item Why are energy systems so complex?
		\asw{\newline interconnected systems, distributed and volatile generation, balancing demand and supply, complex and requires transparency}
		\item Why do we need XAI for energy systems?
		\asw{\newline Operators need to understand and trust. Consumers need to adopt and comply.}
		\item E: What XAI technique is currently predominantly used for Energy Systems?
		\asw{\newline }
		\item What influences the Frequency Stability?
		\asw{\newline The balance of supply and demand.}
		\item What are application for XAI in energy?
		\asw{\newline vulverability of power grid, renewable energy forecasting, building energy management}
		\item What are methods used for interpretability?
		\asw{\newline SHAP, LIME, GradCAM, Attention}
		\item What are current research topic in XAI for energy systems?
		\asw{\newline predict stability of power grid based on external features (generation, load, prices, ...), 
			\newline identify drivers and risks of stability, determine depencies, interactions
			\newline feature ranking, partial depency plots
			\newline identify technology as fast/slow dependent on context}
		\item What are the keytake aways for energy systems?
		\asw{\newline power grids are complex and frequency plays a special role, interpretable ML reveals interaction in world, XAI in energy allows forcasting and insights for a sustainable future}
		\end{itemize}
		\hrule 
	
		\subsection{XAI in Material Science}
		
		\begin{itemize}
		\item What is the difference between knowledge and understanding?
		\asw{\newline }
		\item What is Scientific understanding?
		\asw{\newline }
		\item What are the criteria for Scientific understanding?
		\asw{\newline Understanding phenomena, intelligibility of theories}
		\item What are Aspects of Scientific understanding?
		\asw{\newline Computational Microscope (reveals properties), Resource of inspiration (new concepts/ideas), Agent of understanding (no evidence yet)}
		\item What are conditions for Scientific understanding?
		\asw{\newline 1) can recognize qualitative characteristic consequences of a theory without performing exact computations and use them in a new context
			\newline 2) can transfer its understanding to a human expert}
		\item What is the Scientific Understanding Test?
		\asw{\newline It's similar to the Turing Test for understanding. Student learns from teacher, teacher explains theory. Referee tests teacher and student on task independently, if undistinguishable $\rightarrow$ teacher has scientific understanding}
		
		\item Why do we need data-driven molecule search?
		\asw{\newline exploit and avoid redundancies for speedup and discovery, uncover hidden relations in data for understanding and design}
		\item What is a Self-explaining GNN?
		\asw{\newline It can generate explanations by itself and learn from prior knowledge. There is an "accuracy-explainability tradeoff".}
		\item What type of network is MEGAN?
		\asw{\newline It is a Self-explaining GNN. It is short for Multi-Explanation Graph Attention Network.}
		\item What is the output of MEGAN?
		\asw{\newline The output is the prediction, a node explanation mask and a edge explanation mask.}
		\item What are possible applications for MEGAN?
		\asw{\newline re-discovery of solubility}
		\item What can clustering be used for with GNNs?
		\asw{\newline UMAP, cluster center as generic example explanations}
		\item What are counterfactuals?
		\asw{\newline They look similar than another example, but they have very different properties.}
		\end{itemize}
		\hrule 
	
		\subsection{XAI in Computer Security}
		
		\begin{itemize}
		\item What types of problems are viewed in XAI for computer security?
		\asw{\newline MLSEC: Developing more intelligent security systems (Automation of Attacks/ security)
			\newline SECML: Developing more secure intelligent systems (Attacks against ML, robust ML)}
		\item What are applications of XAI in Computer Security?
		\asw{\newline Detection of malware, software/hardware vulnerabilities, etc.
			\newline Vetting malware behavior: describe what malware does}
		\item How does malware vetting work?
		\asw{\newline derive concise but contextualized explanation of malware
			\newline 1) learn model for groups of malware
			\newline 2) explain using "layer-wise relevance propagation"
			\newline 3) aggreate}
		\item How can the quality of explanations be evaluated?
		\asw{\newline Let users evaluate (extremely difficult), measure using criteria}
		\item What criteria is used for the quality of explanations?
		\asw{\newline General: Descriptive accuracy (accuracy drop, when most relevant features are removed), descriptive sparsity (normalized histogram of relevance $\rightarrow$ sharper $\widehat{=}$ better)
			\newline Security-related: Completeness (derived for all corner cases), stability (do not vary over time), efficiency (runtime performance), robustness (reliable in adversarial settings)}
		\item What is the difference between White-box explanations and Black-box explanations?
		\asw{\newline }
		\item What are different threat models for attacks against ML?
		\asw{\newline Attack against training (model manipulation), attack against prediction (input manipulation)}
		\item E: Name three types of explanation aware attacks
		\asw{\newline Classification preserving, Explanation preserving, Dual Attack}
		\item E: How can we protect a model against explanation aware attacks using trigger patches?
		\asw{\newline We can use Februus or SentiNet to identify and remove trigger patterns. When trigger patterns are identified they can be removed and the image can be inpainted with GANs.}
		\end{itemize}
		\hrule 
	
		\subsection{XAI in Mobility}
		
		\begin{itemize}
		\item What are challenges when explaining AI components?
		\asw{\newline complex models, explanation recipients, holistic explanations for entire system necessary}
		\item E: What are challenges when explaining non-AI components?
		\asw{\newline large number of components, complexity / size of components, different modelling mechanisms of components, (parts of) component's behavior might be hidden (secret)}
		\item What is Self-Explainability?
		\asw{\newline System must be able to explain its own behavior without any outside entity.}
		\item What are the requirements for Self-Explainability?
		\asw{\newline provided spontaneously, during runtime, automatically updated}
		\item E: What is the MAB-EX framework?
		\asw{\newline It is a framework for Self-Explainability. It consists of Monitoring, Analyses, Build explanations, Explain.}
		\item E: What do the components of the MAB-EX framework do?
		\asw{\newline Monitoring: monitor system and environment
			\newline Analyses: analyze monitored data
			\newline Build explanation: retrieve explanation as internal format from explanation model (e.g. Self-Explainable Digital Twins)
			\newline Explain: Transform internal explnation into adequate form, depends on recipient, timing (a priori, in situ, a posteriori), topic}
		\item What is an application for XAI in mobility?
		\asw{\newline Moral Traffic Agents, introduc "common sense" in Autonomous Vehicles}
		\item Why do we need explanations for Moral Traffic Agents?
		\asw{\newline grey scale of unsolvable conflicts $\rightarrow$ let experts decide
			\newline classify potential conflicts, develop solution strategies for each class of conflicts}
		\end{itemize}
		\hrule 
	
		\subsection{XAI in NLP}
		
		\begin{itemize}
		\item What type of models and explanations are used for Machine Translation?
		\asw{\newline Transformer networks with Attention as explanation}
		\item How can attention-based models be interpreted?
		\asw{\newline gradient-based methods, using attention-score (problem: multiple attention blocks), using token contribution, contrastive explanations (motivation: non-contrastive have most attention on immediately preceeding token)}
		\item What are the goals of Perturbation-based Quality Estimation?
		\asw{\newline main goal: Quality Estimation, side goal: explainability}
		\item What are the properties of Perturbation-based Quality Estimation?
		\asw{\newline model-agnostic, works for black-box MT systems}
		\item E: Briefly Describe the two main steps of Pertubation-based Quality Estimation in NLP.
		\asw{\newline 1) Perturb each word $x_i$ in source sentence
			\newline 2) if output word $y_j$ changes as we perturb $x_i$ $\Rightarrow$ $x_i$ influences $y_j$}
		\end{itemize}
	\newpage
	
	\section{Neural Network Interpretation}
	
	\begin{itemize}
		\item What is the disadvantage of using model-agnostic post-hoc methods like Global Surrogates, LIME, SHAP, etc.?
		\asw{\newline Main reason DNNs are so powerful: ability to learn/extract features
			\newline $\rightarrow$ special method needed to interpret these
			\newline gradients can be used to generate interpretation more efficiently}
		
		\item What is \textbf{\underline{Feature Visualization}}? Why do we need it?
		\asw{\newline The aim is to visualize general concepts of learned features that triggers a particular set of activations.
			\newline It helps to understand and interpret the used features. It also helps to understand the model itself in general.
			\newline The idea of Feature Visualization is to try to find inputs that maximize these feature values.}
		\item What are approaches to do Feature Visualization?
		\asw{\newline Find training images that has maximal activation of the feature.
			\newline $\rightarrow$ problem: which part of image is responsible for high activation?
			\newline Start from random noise and optimize image stepwise.
			\newline $\rightarrow$ requires constraint on optimization (small steps), regularization like jittering, rotation, scaling to reduce noise in visualization}
		\item How can interactions between learned features be visualized?
		\asw{\newline Through the use of the concept of direction in activation space. Elements of direction vector are continuous valued and can be negative.
			\newline $h(x) = d^T a_x$ ($d$: direction, $a_x$: activations)
			\newline $\rightarrow$ possible to interpolate between activations}
		\item What are the advantages / disadvantages of Feature Visualization?
		\asw{\newline \textcolor{green}{$+$} unique insight into NNs
			\newline \textcolor{green}{$+$} communication non-technical
			\newline \textcolor{green}{$+$} combine with feature attribution methods
			\newline \textcolor{green}{$+$} not limited to images
			\newline \textcolor{red}{$-$} often not interpretable
			\newline \textcolor{red}{$-$} illusion of interpretability
			\newline \textcolor{red}{$-$} too many units to look at}
		
		\item What is \textbf{\underline{Network Dissection}}?
		\asw{\newline The idea is to find a concept that is closer to human understanding.}
		\item What types of labels are used for Network Dissection?
		\asw{\newline Pixel-level labels of general human concepts}
		\item What is the quality metric for Network Dissection?
		\asw{\newline An IoU score gets computed and needs to be over a certain threshold ($IoU_{k,c}>0.04$ $\rightarrow$ channel k is detector for concept c)}
		\item E: Briefly describe the three main steps of Network Dissecton.
		\asw{\newline given: image and trained network
			\newline 1) Pass image through network
			\newline 2) Upscale activations to match original image size
			\newline 3) (Later) compare max activations to ground-truth pixel-wise segmentation}
		\item What are the advantages / disadvantages of Network Dissection?
		\asw{\newline \textcolor{green}{$+$} automatically link concepts
			\newline \textcolor{green}{$+$} detect concepts beyond classes
			\newline \textcolor{red}{$-$} requires dataset labeled on pixel-level with concepts
			\newline \textcolor{red}{$-$} only positive activations aligned with human concepts}
		
		\item What is \textbf{\underline{Pixel Attribution}}?
		\asw{\newline It is a type of feature attribution for images.
			\newline Question: "What part of the image lead the models decision?"}
		\item How does Pixel Attribution work?
		\asw{\newline It uses the gradient to determine how big the influence of a specific pixel is on the output.}
		\item What are \textbf{Saliency Maps}?
		\asw{\newline They show which pixels of the input images are important for the output of a network.}
		\item What is Saliency?
		\asw{\newline Saliency describes the gradient of the output.}
		\item What kinds of Saliencies are often visualized?
		\asw{\newline absolute, positive, negative values of Saliency}
		\item E: Which part of a Convolution Stage in a CNN might cause problems for saliency maps and why?
		\asw{\newline The activation function often causes problems. With the ReLU activation function all of the values lower than 0 are mapped to 0. Therefore the information on where this value came from is lost and can not be backpropagated anymore.
			\newline A solution for this would be to use the indicator function in the backpropagation.}
		\item E: Name one advantage and one disadvantage for Saliency Maps.
		\asw{\newline \textcolor{green}{$+$} faster to compute than model-agnostic methods
			\newline \textcolor{red}{$-$} methods are dependent on human interpretation of pixel importance}
		\item What is \textbf{GradCAM}?
		\asw{\newline It is a method that generates explanations for CNN decisions.}
		\item What is the algorithm for GradCAM?
		\asw{\newline 1) Pass input image through CNN, only use raw score for class of interest $\rightarrow$ other activations $=0$
			\newline 2) Backprop gradient to last conv layer before FC layer
			\newline 3) Weight feature map pixel by gradient for class (globally pooled gradients)
			\newline 4) Calculate average of feature maps, weighted per pixel by gradient \& apply ReLU to average
			\newline 5) For visualization: scale values to [0,1], upscale image \& overlay over original image}
		\item What are advantages / disadvantages for Pixel Attributions?
		\asw{\newline \textcolor{green}{$+$} explanations are visual, quick to recognize images
			\newline \textcolor{green}{$+$} faster to compute than model-agnostic methods
			\newline \textcolor{green}{$+$} many methods to choose from: GradCAM, Guided GradCAM, SmoothGrad
			\newline \textcolor{red}{$-$} difficult to know whether explanation is correct (qualitative only)
			\newline \textcolor{red}{$-$} Pixel Attribution methods can be very fragile
			\newline \textcolor{red}{$-$} methods can be highly unreliable
			\newline \textcolor{red}{$-$} (potentially) insensitive to model \& data}
	\end{itemize}
	
\end{document}
